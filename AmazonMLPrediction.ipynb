{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AmazonMLPrediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LuUPT_5cVNQ",
        "outputId": "f2e8875a-4f07-4455-e9ee-c7ebd26db01f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCb1fQZ7tPkd"
      },
      "source": [
        "import pandas as pd\n",
        "import csv"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMVEFEghtnmW",
        "outputId": "72f21be0-887f-475c-f843-7f226dece116"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW7q-9i2tbXA"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
        "\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return \" \".join(result)+\" \""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBouED5JtcM_"
      },
      "source": [
        "def process(df):  \n",
        "  processed_docs = df['TITLE']+\" \"+df['DESCRIPTION']+\" \"+df['BULLET_POINTS']+\" \"+df['BRAND']\n",
        "  return processed_docs.astype(str).map(preprocess)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS0eA2dKlGhy",
        "outputId": "931f93b3-130f-4e7a-f511-cf9b808800a0"
      },
      "source": [
        "chunksize = (10 ** 4)\n",
        "filename = \"/content/gdrive/MyDrive/AmazonMLChallenge/test.csv\"\n",
        "cols=[\"DOCUMENT\"]\n",
        "chunk_number=0\n",
        "for chunk in pd.read_csv(filename, chunksize=chunksize, escapechar = \"\\\\\", na_values='' ,quoting = csv.QUOTE_NONE):\n",
        "  chunk_number+=1 \n",
        "  chunk.fillna(\" \",inplace=True)\n",
        "  print(\"Processing Chunk:\",chunk_number)\n",
        "  processed_chunk = pd.DataFrame()  \n",
        "  processed_chunk[\"DOCUMENT\"] = process(chunk)  \n",
        "  if chunk_number==1:\n",
        "    processed_chunk.to_csv('/content/gdrive/MyDrive/AmazonMLChallenge/final_processed_test.txt',index = False, sep = ' ',header = None, quoting = csv.QUOTE_NONE, quotechar = \"\", escapechar = \" \")\n",
        "  else:    \n",
        "    processed_chunk.to_csv('/content/gdrive/MyDrive/AmazonMLChallenge/final_processed_test.txt', mode='a', index = False, sep = ' ',header = None, quoting = csv.QUOTE_NONE, quotechar = \"\", escapechar = \" \")    \n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Chunk: 1\n",
            "Processing Chunk: 2\n",
            "Processing Chunk: 3\n",
            "Processing Chunk: 4\n",
            "Processing Chunk: 5\n",
            "Processing Chunk: 6\n",
            "Processing Chunk: 7\n",
            "Processing Chunk: 8\n",
            "Processing Chunk: 9\n",
            "Processing Chunk: 10\n",
            "Processing Chunk: 11\n",
            "Processing Chunk: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj8j5kiXk7Q3",
        "outputId": "78e25dd1-7e2d-4d42-8a40-fdb498605705"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 40.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.7.0-py2.py3-none-any.whl (199 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3093456 sha256=95e277ecd74903d9ca69803a68fd66d30b8655cbfe8f12f4bf4d3dcdbbea577c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZA7PKWVk587"
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9hRTo-yqSj_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFZYYaZWkEhg",
        "outputId": "eb385128-18f9-422a-cb99-f2a1c5bce64e"
      },
      "source": [
        "# Or load pre-trained classifier\n",
        "classifier = fasttext.load_model('/content/gdrive/MyDrive/AmazonMLChallenge/model.bin') #label_prefix='__label__')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VpJJsUXzpC9"
      },
      "source": [
        "results=[]\n",
        "file1 = open('myfile.txt', 'r')\n",
        "Lines = file1.readlines()\n",
        " \n",
        "count = 0\n",
        "# Strips the newline character\n",
        "for line in Lines:\n",
        "    count += 1\n",
        "    #print(\"Line{}: {}\".format(count, line.strip()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GizwnQF4zOK8"
      },
      "source": [
        "sub=pd.DataFrame()\n",
        "sub['PRODUCT_ID']="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2bVBqm8knBT"
      },
      "source": [
        "classifier.predict()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}